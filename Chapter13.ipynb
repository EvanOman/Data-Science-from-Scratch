{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Chapter 13: Naive Bayes\n",
    "\n",
    "### Some Theory\n",
    "\n",
    "Here we will build a Naive Bayes Classifier for the Spam/Ham classification problem. Naive Baye is built on Bayes theorem, as given below:\n",
    "\n",
    "$P(A|B) = \\displaystyle \\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|\\neg A)P(\\neg A)} $\n",
    "\n",
    "In the email classification domain we would treat the event $A$ as the \"this email is spam\" and then the event $B$ as some feature of the given email.\n",
    "\n",
    "A common $B$ feature to use would be the existance of a certain word. Thus if we set some event $V$ as \"the message contains the word viagra\", then $P(A|V)$ would be the probability that an email is spam given that it contains the word \"Viagra\". \n",
    "\n",
    "Key passage:\n",
    "> The key to Naive Bayes is making the (big) assumption that the precences (or absences) of each word are independent of one another, conditional on the message being spam or not.\n",
    "\n",
    "Given this independence assumption we can make take the following mathematical step:\n",
    "\n",
    "$\\displaystyle P(X_1 = x_1, \\ldots, X_n = x_n | S) = P(X_1=x_1|S) \\times \\cdots \\times P(X_n=x_n|S) = \\prod_{i=1}^n P(X_i=x_i|S)$\n",
    "\n",
    "Thus applying Bayes Theorem we can calculate the probability of a message being spam or not by simply coming up with an estimate for $P(X_i|S)$ and $P(X_i|\\neg S)$. Estimating this is then as simple as counting words from labeled spam/ham messages, given the following \"smoothing\" measure to account for missing data (called _pseudocounting_):\n",
    "\n",
    "$P(X_i|S) = \\displaystyle \\frac{k + \\text{ number of spams containing } w_i}{2k + \\text{number of spams}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, k=0.5):\n",
    "        self.k = k\n",
    "        self.word_probs = {}\n",
    "        \n",
    "    def train(self, training_set):\n",
    "        \n",
    "        # count spam and non-spam messages\n",
    "        num_spams = len([is_spam\n",
    "                        for message, is_soan in training_set\n",
    "                        if is_spam])\n",
    "        num_non_spams = len(training_set) - num_spams\n",
    "        \n",
    "        # run training data through our \"pipeline\"\n",
    "        word_counts = count_words(training_set)\n",
    "        self.word_probs = word_probabilities(word_counts, \n",
    "                                             num_spams, \n",
    "                                             num_non_spams,\n",
    "                                             self.k)\n",
    "        \n",
    "    def classify(self, message):\n",
    "        return spam_probability(self.word_probs, message)\n",
    "        \n",
    "    \n",
    "    # Static methods\n",
    "    def tokenize(message):\n",
    "        message = message.lower()                      # convert to lowercase\n",
    "        all_words = re.findall(\"[a-z0-9']+\", message)  # extract the words\n",
    "        return set(all_words)                          # remove duplicates\n",
    "    \n",
    "    def count_words(training_set):\n",
    "        \"\"\"training set consists of pairs (message, is_spam)\"\"\"\n",
    "        counts = defaultdict(lambda: [0, 0])\n",
    "        for message, is_spam in training_set:\n",
    "            for word in tokenize(message):\n",
    "                counts[word][0 if is_spam else 1] += 1\n",
    "        return counts\n",
    "    \n",
    "    def word_probabilities(counts, total_spams, total_non_spams, k=0.5):\n",
    "        \"\"\"\n",
    "            turn the word_counts into a list of triplets:\n",
    "            (w, p(w | spam), p(w | -spam))\n",
    "        \"\"\"\n",
    "        return [(w,\n",
    "                (spam + k) / (total_spams + 2*k),\n",
    "                (non_spam + k) / (total_non_spams + 2*k))\n",
    "               for w, (spam, not_spam) in counts.iteritems()]\n",
    "    \n",
    "    def spam_probability(word_probs, message):\n",
    "        message_words = tokenize(message)\n",
    "        log_prob_if_spam = log_prob_if_not_spam = 0.0\n",
    "        \n",
    "        # iterate through each word in our vocab\n",
    "        for word, prob_if_spam, prob_if_not_spam in word_probs:\n",
    "            \n",
    "            # if *word* appears in the message, add the log prob of seeing it\n",
    "            if word in message_words:\n",
    "                log_prob_if_spam += math.log(prob_if_spam)\n",
    "                log_prob_if_not_spam += math.log(prob_if_not_spam)\n",
    "                \n",
    "            # if *word* doesn't appear in the message, add the prob of not seeing it\n",
    "            else:\n",
    "                log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
    "                log_prob_if_not_spam += math.log(1.0 - prob_if_not_spam)\n",
    "                \n",
    "        prob_if_spam = math.exp(log_prob_if_spam)\n",
    "        prob_if_not_spam = math.exp(log_prob_if_not_spam)\n",
    "        return prob_if_spam / (prob_if_spam + prob_if_not_spam)              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODOS\n",
    "- Run on some training data\n",
    "- Plot the ROC curve\n",
    "    - Vary the spam probability cutoff\n",
    "    - Use a color map for the probability value correponding to each point in the curve"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
