{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Chapter 13: Naive Bayes\n",
    "\n",
    "### Some Theory\n",
    "\n",
    "Here we will build a Naive Bayes Classifier for the Spam/Ham classification problem. Nai                                                                                                 ve Baye is built on Bayes theorem, as given below:\n",
    "\n",
    "$P(A|B) = \\displaystyle \\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|\\neg A)P(\\neg A)} $\n",
    "\n",
    "In the email classification domain we would treat the event $A$ as the \"this email is s                                                                                                 pam\" and then the event $B$ as some feature of the given email.\n",
    "\n",
    "A common $B$ feature to use would be the existance of a certain word. Thus if we set som                                                                                                 e event $V$ as \"the message contains the word viagra\", then $P(A|V)$ would be the probabili                                                                                                 ty that an email is spam given that it contains the word \"Viagra\". \n",
    "\n",
    "Key passage:\n",
    "> The key to Naive Bayes is making the (big) assumption that the precences (or absences)                                                                                                  of each word are independent of one another, conditional on the message being spam or not.\n",
    "                                                                                                 \n",
    "Given this independence assumption we can make take the following mathematical step:\n",
    "\n",
    "$\\displaystyle P(X_1 = x_1, \\ldots, X_n = x_n | S) = P(X_1=x_1|S) \\times \\cdots \\times P(X_n=x_n|S) = \\prod_{i=1}^n P(X_i=x_i|S)$\n",
    "\n",
    "Thus applying Bayes Theorem we can calculate the probability of a message being spam or                                                                                                  not by simply coming up with an estimate for $P(X_i|S)$ and $P(X_i|\\neg S)$. Estimating this                                                                                                  is then as simple as counting words from labeled spam/ham messages, given the following \"sm                                                                                                 oothing\" measure to account for missing data (called _pseudocounting_):\n",
    "\n",
    "$P(X_i|S) = \\displaystyle \\frac{k + \\text{ number of spams containing } w_i}{2k + \\text{number of spams}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, k=0.5):\n",
    "        self.k = k\n",
    "        self.word_probs = []\n",
    "\n",
    "    def train(self, training_set):\n",
    "    \n",
    "        # count spam and non-spam messages\n",
    "        num_spams = len([is_spam \n",
    "                         for message, is_spam in training_set \n",
    "                         if is_spam])\n",
    "        num_non_spams = len(training_set) - num_spams\n",
    "\n",
    "        # run training data through our \"pipeline\"\n",
    "        word_counts = NaiveBayesClassifier.count_words(training_set)\n",
    "        self.word_probs = NaiveBayesClassifier \\\n",
    "                            .word_probabilities(word_counts, \n",
    "                                                 num_spams, \n",
    "                                                 num_non_spams,\n",
    "                                                 self.k)\n",
    "        \n",
    "    def classify(self, message):\n",
    "        return NaiveBayesClassifier.spam_probability(self.word_probs, message)\n",
    "        \n",
    "    \n",
    "    # Static methods\n",
    "    def tokenize(message):\n",
    "        message = message.lower()                       # convert to lowercase\n",
    "        all_words = re.findall(\"[a-z0-9']+\", message)   # extract the words\n",
    "        return set(all_words)                           # remove duplicates\n",
    "\n",
    "\n",
    "    def count_words(training_set):\n",
    "        \"\"\"training set consists of pairs (message, is_spam)\"\"\"\n",
    "        counts = defaultdict(lambda: [0, 0])\n",
    "        for message, is_spam in training_set:\n",
    "            for word in NaiveBayesClassifier.tokenize(message):\n",
    "                counts[word][0 if is_spam else 1] += 1\n",
    "        return counts\n",
    "\n",
    "    def word_probabilities(counts, total_spams, total_non_spams, k=0.5):\n",
    "        \"\"\"turn the word_counts into a list of triplets \n",
    "        w, p(w | spam) and p(w | ~spam)\"\"\"\n",
    "        return [(w,\n",
    "                 (spam + k) / (total_spams + 2 * k),\n",
    "                 (non_spam + k) / (total_non_spams + 2 * k))\n",
    "                 for w, (spam, non_spam) in counts.items()]\n",
    "\n",
    "    def spam_probability(word_probs, message):\n",
    "        message_words = NaiveBayesClassifier.tokenize(message)\n",
    "        log_prob_if_spam = log_prob_if_not_spam = 0.0\n",
    "        \n",
    "        # iterate through each word in our vocab\n",
    "        for word, prob_if_spam, prob_if_not_spam in word_probs:\n",
    "\n",
    "            # for each word in the message, \n",
    "            # add the log probability of seeing it \n",
    "            if word in message_words:\n",
    "                log_prob_if_spam += math.log(prob_if_spam)\n",
    "                log_prob_if_not_spam += math.log(prob_if_not_spam)\n",
    "\n",
    "            # for each word that's not in the message\n",
    "            # add the log probability of _not_ seeing it\n",
    "            else:\n",
    "                log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
    "                log_prob_if_not_spam += math.log(1.0 - prob_if_not_spam)\n",
    "                \n",
    "        prob_if_spam = math.exp(log_prob_if_spam)\n",
    "        prob_if_not_spam = math.exp(log_prob_if_not_spam)\n",
    "        return prob_if_spam / (prob_if_spam + prob_if_not_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using some data\n",
    "\n",
    "Using the SpamAssassin public corpus (use `./DownloadCh13SpamData.sh` to download the da                                                                                                 ta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Chapter11.ipynb\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import glob, re, sys, random\n",
    "    import NotebookLoader\n",
    "    sys.meta_path.append(NotebookLoader.NotebookFinder())\n",
    "    import Chapter11 as CH11\n",
    "    \n",
    "    path = r\"./Ch13_data/*/*\"\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    # use glob.glob to return every filename that matches our path wildcard\n",
    "    for fn in glob.glob(path):\n",
    "        is_spam = \"ham\" not in fn\n",
    "        \n",
    "        with open(fn,'r',encoding = \"ISO-8859-1\") as file:\n",
    "            for line in file:\n",
    "                if line.startswith(\"Subject:\"):\n",
    "                    # remove the leading \"Subject:\" and keep what is left\n",
    "                    subject = re.sub(r\"^Subject: \", \"\", line).strip()\n",
    "                    data.append((subject, is_spam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spam vs Ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 503 Spam\n",
      "There are 2920 Ham\n",
      "This gives a spam-ham ratio of 0.17226027397260274\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    numSpam = sum([1 for _,isSpam in data if isSpam])\n",
    "    numHam = len(data) - numSpam\n",
    "    print(\"There are {0} Spam\\nThere are {1} Ham\".format(numSpam, numHam))\n",
    "    print(\"This gives a spam-ham ratio of {0}\".format(numSpam/numHam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set seed for consistency\n",
    "    random.seed(0) \n",
    "    \n",
    "    train_data, test_data = CH11.split_data(data, 0.75)\n",
    "    \n",
    "    classifier = NaiveBayesClassifier()\n",
    "    classifier.train(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(False, False): 717, (True, True): 86, (True, False): 48, (False, True): 25})\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # make triplets: (subject, actual is_spam, predicted is_spam prob)\n",
    "    classified = [(subject, is_spam, classifier.classify(subject))\n",
    "                  for subject, is_spam in test_data]\n",
    "    \n",
    "    # assume 0.5 probability cutoff, count combinations of (is_spam, pred_is_spam)\n",
    "    counts = Counter((is_spam, spam_prob > 0.5) \n",
    "                     for _, is_spam, spam_prob in classified)\n",
    "    \n",
    "    print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull out Accuracy, Precision, Recall, and $F_1$ Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 717\n",
      "True Positives: 86\n",
      "False Negatives: 48\n",
      "False Positves: 25\n",
      "Accuracy: 0.9166666666666666\n",
      "Precision: 0.7747747747747747\n",
      "Recall: 0.6417910447761194\n",
      "F1 Score: 0.7020408163265306\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    TN = counts[(False, False)]\n",
    "    TP = counts[(True, True)]\n",
    "    FN = counts[(True, False)]\n",
    "    FP = counts[(False, True)]\n",
    "    \n",
    "    print(\"True Negatives: {0}\".format(TN))\n",
    "    print(\"True Positives: {0}\".format(TP))\n",
    "    print(\"False Negatives: {0}\".format(FN))\n",
    "    print(\"False Positves: {0}\".format(FP))\n",
    "    \n",
    "    print(\"Accuracy: {0}\".format(CH11.accuracy(TP, FP, FN, TN)))\n",
    "    print(\"Precision: {0}\".format(CH11.precision(TP, FP, FN, TN)))\n",
    "    print(\"Recall: {0}\".format(CH11.recall(TP, FP, FN, TN)))\n",
    "    print(\"F1 Score: {0}\".format(CH11.f1_score(TP, FP, FN, TN)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spammy Hams and Hammy Spams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spammiest Hams: [('FREE SHIPPING! No Minimum Purchase* at Buy.com', False, 0.997304337342074), ('[ILUG-Social] Re: Important - reenactor insurance needed', False, 0.9983860089741922), ('=?iso-8859-1?Q?Matrox_Parhelia=99_now_available?=', False, 0.9996572307935708), ('=?iso-2022-jp?B?UmU6IBskQjswSSkyPTNYJSglcyU4JUslIiVqJXMlME1NJVcbKEI=?=', False, 0.9999875315335071), ('=?iso-2022-jp?B?GyRCRnxLXDhsJE43b0w+IUolNSVWJTglJyUvJUghSyEhJTkbKEI=?=', False, 0.9999875315335071)]\n",
      "\n",
      "\n",
      "Hammiest Spams: [('Re: Hi', True, 0.0004378519507496577), ('Testing a system, please delete', True, 0.001485592401614698), ('Tired of paying big bucks for cable', True, 0.0016459111795937217), ('*****SPAM*****', True, 0.0018341715264413787), ('.Message report from your contact page....//ytu855 rkq', True, 0.0028724699034063782)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # sort by spam prob (ascending)\n",
    "    classified.sort(key = lambda row: row[2])\n",
    "    \n",
    "    # highest predicted spam probabilities among the non-spams\n",
    "    spammiest_hams = list(filter(lambda row: not row[1], classified))[-5:]\n",
    "    \n",
    "    # the lowest predict spam probabilities among the actual spams\n",
    "    hammiest_spams = list(filter(lambda row: row[1], classified))[:5]\n",
    "    \n",
    "    print(\"Spammiest Hams: {0}\\n\\n\".format(spammiest_hams))\n",
    "    print(\"Hammiest Spams: {0}\\n\\n\".format(hammiest_spams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spammiest words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spammiest Words: [('zzzz', 0.031081081081081083, 0.0002294630564479119), ('systemworks', 0.031081081081081083, 0.0002294630564479119), ('money', 0.041891891891891894, 0.0002294630564479119), ('adv', 0.041891891891891894, 0.0002294630564479119), ('rates', 0.0445945945945946, 0.0002294630564479119)]\n",
      "\n",
      "\n",
      "Hammiest Words: [('satalk', 0.0013513513513513514, 0.047957778797613586), ('spambayes', 0.0013513513513513514, 0.04703992657182194), ('users', 0.0013513513513513514, 0.040614960991280404), ('razor', 0.0013513513513513514, 0.03510784763653052), ('zzzzteana', 0.0013513513513513514, 0.02914180816888481)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def p_spam_given_word(word_prob):\n",
    "    \"\"\"uses bayes's theorem to compute p(spam | message contains word)\"\"\"\n",
    "    # word_prob is one of the triplets produced by word_probabilities\n",
    "    word, prob_if_spam, prob_if_not_spam = word_prob\n",
    "    return prob_if_spam / (prob_if_spam + prob_if_not_spam)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    words = sorted(classifier.word_probs, key=p_spam_given_word)\n",
    "    print(\"Spammiest Words: {0}\\n\\n\".format(words[-5:]))\n",
    "    print(\"Hammiest Words: {0}\\n\\n\".format(words[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODOS\n",
    "- Run on some training data\n",
    "- Plot the ROC curve\n",
    "    - Vary the spam probability cutoff\n",
    "    - Use a color map for the probability value correponding to each point in the curve"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
