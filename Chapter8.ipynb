{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Chapter4.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Chapter 8 from Data Science from Scratch\n",
    "from collections import Counter\n",
    "#from linear_algebra import distance, vector_subtract, scalar_multiply\n",
    "from functools import reduce\n",
    "import math, random\n",
    "import sys\n",
    "import NotebookLoader\n",
    "sys.meta_path.append(NotebookLoader.NotebookFinder())\n",
    "import Chapter4 as Ch4\n",
    "\n",
    "isMain = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sum_of_squares(v):\n",
    "    \"\"\"computes the sum of squared elements in v\"\"\"\n",
    "    return sum(v_i ** 2 for v_i in v)\n",
    "\n",
    "\n",
    "def difference_quotient(f, x, h):\n",
    "    return (f(x+h) - f(x))/h\n",
    "\n",
    "\n",
    "def square(x):\n",
    "    return x*x\n",
    "\n",
    "\n",
    "def derivative(x):\n",
    "    return 2*x\n",
    "\n",
    "if isMain:\n",
    "    derivative_est = lambda x: difference_quotient(square, x, h=.00001)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    x = range(-10,10)\n",
    "    plt.title(\"Actual Derivative vs Esitmates\")\n",
    "    plt.plot(x, [derivative(x_i) for x_i in x], 'rx', label='Actual')\n",
    "    plt.plot(x, [derivative_est(x_i) for x_i in x], 'b+', label='Estimate')\n",
    "    plt.legend(loc=9)\n",
    "    plt.draw()\n",
    "\n",
    "\n",
    "def partial_difference_quotient(f, v, i, h):\n",
    "    \"\"\"compute the ith partial difference quotient of f at v\"\"\"\n",
    "    w = [v_j + (h if j == i else 0) for j, v_j in enumerate(v)]\n",
    "    return (f(w) - f(v)) / h\n",
    "\n",
    "\n",
    "def estimate_gradient(f, v, h=.00001):\n",
    "    return [partial_difference_quotient(f, v, i, h) for i, _ in enumerate(v)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously sum of squares has minimum at $v = \\{0\\}$ but lets find this using gradient descent\n",
    "\n",
    "Gradient Descent: Choose random starting point, take steps in the direction of greatest down slope (minimum direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approx solutions:  [-1.2794544714561508e-07, 2.1324241190935878e-07, -4.2648482381871755e-07]\n"
     ]
    }
   ],
   "source": [
    "def step(v, direction, step_size):\n",
    "    \"\"\"move step_size in the direction from v\"\"\"\n",
    "    return [v_i + step_size * direction_i for v_i, direction_i in zip(v, direction)]\n",
    "\n",
    "\n",
    "def sum_of_squares_gradient(v):\n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "if isMain:\n",
    "    # pick a random starting point\n",
    "    v = [random.randint(-10, 10) for _ in range(3)]\n",
    "\n",
    "    tolerance = .00000001\n",
    "\n",
    "    while True:\n",
    "        gradient = sum_of_squares_gradient(v)\n",
    "        next_v = step(v, gradient, -0.01)\n",
    "        if Ch4.distance(next_v, v) < tolerance:\n",
    "            break\n",
    "        v = next_v\n",
    "    print(\"Approx solutions: \", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But what should the step size be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a safe apply function, just in case we try a bad step size\n",
    "def safe(f):\n",
    "    \"\"\"\n",
    "        Return a new function that is the same as f, except\n",
    "        that it outputs infinity whenever f produces an error\n",
    "    \"\"\"\n",
    "    def safe_f(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except:\n",
    "            return float('inf')\n",
    "    return safe_f\n",
    "\n",
    "\n",
    "# Supposing that we have chosen some initial param theta_0, we can implement gradient descent as:\n",
    "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=.0000001):\n",
    "    \"\"\"\n",
    "        Use gradient descent to find the theta that minimizes target function\n",
    "    \"\"\"\n",
    "\n",
    "    step_sizes = [10**(2-i) for i in range(0,8)]\n",
    "\n",
    "    theta = theta_0\n",
    "    target_fn = safe(target_fn)\n",
    "    value = target_fn(theta)\n",
    "\n",
    "    while True:\n",
    "        gradient = gradient_fn(theta)\n",
    "        next_thetas = [step(theta, gradient, -step_size) for step_size in step_sizes]\n",
    "\n",
    "        # Choose the one that minimizes the error function\n",
    "        next_theta = min(next_thetas, key=target_fn)\n",
    "        next_value = target_fn(next_theta)\n",
    "\n",
    "        # Stop if we're \"converging\"\n",
    "        if abs(value - next_value) < tolerance:\n",
    "            return theta\n",
    "        else:\n",
    "            theta, value = next_theta, next_value\n",
    "\n",
    "\n",
    "def negate(f):\n",
    "    \"\"\"Return a function that for any input x, returns -f(x)\"\"\"\n",
    "    return lambda *args, **kwargs: -f(*args, **kwargs)\n",
    "\n",
    "\n",
    "def negate_all(f):\n",
    "    \"\"\"the same when f returns a list of numbers\"\"\"\n",
    "    return lambda *args, **kwargs: [-y for y in f(*args, **kwargs)]\n",
    "\n",
    "\n",
    "def maximize_batch(target_fn, gradient_fn, theta_0, tolerance=.0000001):\n",
    "    return minimize_batch(negate(target_fn), negate_all(gradient_fn), theta_0, tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can use stochastic gradient descent when the error functions are additive. This is better because it saves time on each step. Stochastic gradient descent computes the gradient for only one point at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb4f2053dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# During each cycle we want to iterate through the data in a random order\n",
    "def in_random_order(data):\n",
    "    \"\"\"generator that returns the elements of data in random order\"\"\"\n",
    "    indexes = [i for i,_ in enumerate(data)] # create a list of indexes\n",
    "    random.shuffle(indexes)\n",
    "\n",
    "    # return the data in this shuffled order\n",
    "    for i in indexes:\n",
    "        yield data[i]\n",
    "\n",
    "\n",
    "# Using the random order, we take a gradient step for each data point\n",
    "def minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=.01):\n",
    "\n",
    "    data = zip(x,y)\n",
    "    theta = theta_0\n",
    "    alpha = alpha_0\n",
    "    min_theta, min_value = None, float(\"inf\")\n",
    "    iterations_with_no_improvement = 0\n",
    "\n",
    "    # If we ever go to 100 iterations with no improvement, stop\n",
    "    while iterations_with_no_improvement < 100:\n",
    "        value = sum(target_fn(x_i, y_i, theta) for x_i, y_i in data)\n",
    "\n",
    "        if value < min_value:\n",
    "            # if we've found a new minimum, remember it\n",
    "            # and go back to the original step size\n",
    "            min_theta, min_value = theta, value\n",
    "            iterations_with_no_improvement = 0\n",
    "            alpha = alpha_0\n",
    "        else:\n",
    "            # otherwise we're no improving, so we should try shrinking the step size\n",
    "            iterations_with_no_improvement += 1\n",
    "            alpha *= .9\n",
    "\n",
    "        # ad take a gradient step for each of the datapoints\n",
    "        for x_i, y_i in in_random_order(data):\n",
    "            gradient_i = gradient_fn(x_i, y_i, theta)\n",
    "            theta = Ch4.vector_subtract(theta, Ch4.scalar_multiply(alpha, gradient_i))\n",
    "\n",
    "    return min_theta\n",
    "\n",
    "def maximize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=.01):\n",
    "    return minimize_stochastic(negate(target_fn), negate_all(gradient_fn), x, y, theta_0, alpha_0)\n",
    "\n",
    "if isMain:\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
